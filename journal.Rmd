---
title: "Journal (reproducible report)"
author: "Stefan Neumann"
date: "2020-11-05"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

# Chapter 2: Introduction to the tidyverse

## Bike Sales by Location

In the following you can find code, which analyzes the cummulated bike sales from 2015 to 2020 by state.

Last compiled: `r Sys.Date()`

```{r, fig.width=10, fig.height=7}
# Load libraries ----

library(tidyverse)
library(readxl)

# Importing Files ----

bikes_tbl <- read_excel(path = "data-science/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
orderlines_tbl <- read_excel("data-science/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")
bikeshops_tbl  <- read_excel("data-science/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")

# Joining Data ----

bike_orderlines_joined_tbl <- orderlines_tbl %>%
  left_join(bikes_tbl, by = c("product.id" = "bike.id")) %>%
  left_join(bikeshops_tbl, by = c("customer.id" = "bikeshop.id"))
 
# Wrangling Data ----

bike_orderlines_wrangled_tbl <- bike_orderlines_joined_tbl %>%

  # Separate state and city
  separate(col    = location,
           into   = c("city", "state"),
           sep    = ", ") %>%
  
  # Only select the important columns
  select(quantity, price, order.date, city, state) 



# Sales by Location ----

library(lubridate)

sales_by_loc_tbl <- bike_orderlines_wrangled_tbl %>%

  # Add total price and year column 
  mutate(total_price = quantity * price) %>%
  mutate(year = year(order.date)) %>%

  # Group table (internally) by state for consequent summary
  group_by(state) %>%
  
  # Remove some columns
  select(-quantity, -price, -order.date, -city) %>%

  # Add every sale up for the respective states
  # This only works because we used the group_by command!
  summarize(sales = sum(total_price)) %>%

  # Add a column that turns the numbers into a currency format
  mutate(sales_text = scales::dollar(sales, big.mark = ".",
                                     decimal.mark = ",",
                                     prefix = "",
                                     suffix = " €"))

# Visualization

sales_by_loc_tbl %>%

  # Setup canvas with the columns state (x-axis) and sales (y-axis)
  ggplot(aes(x = state, y = sales)) +

  # Fill the bars with color
  geom_col(fill = "#2DC6D6") +
  
  #Put labels ON TOP OF the bars
  geom_label(aes(label = sales_text)) +
  
  # Rotate text to fit long names
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +

  # Modify y axis labels 
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".",
                                                    decimal.mark = ",",
                                                    prefix = "",
                                                    suffix = " €")) +
  
  # and last but not least add the general titles and labels for the plot
  labs(
    title    = "Revenue by state",
    x = "State", # Override defaults for x and y
    y = "Revenue"
  )

```

## Bike Sales by Location and Year

The following code analyzes bike sales by state and year, so we get 12 individual plots for 12 states and each plot contains data over the last five years.

Last compiled: `r Sys.Date()`

```{r, fig.width=10, fig.height=7}
# Load libraries ----

library(tidyverse)
library(readxl)

# Importing Files ----

bikes_tbl <- read_excel(path = "data-science/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
orderlines_tbl <- read_excel("data-science/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")
bikeshops_tbl  <- read_excel("data-science/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")

# Joining Data ----

bike_orderlines_joined_tbl <- orderlines_tbl %>%
  left_join(bikes_tbl, by = c("product.id" = "bike.id")) %>%
  left_join(bikeshops_tbl, by = c("customer.id" = "bikeshop.id"))
 
# Wrangling Data ----

bike_orderlines_wrangled_tbl <- bike_orderlines_joined_tbl %>%

  # Separate location into city and state
  separate(col    = location,
           into   = c("city", "state"),
           sep    = ", ") %>%

   select(quantity, price, order.date, city, state) 

# Sales by Location And Year----

library(lubridate)

sales_by_year_and_loc_tbl <- bike_orderlines_wrangled_tbl %>%

  # Add year and total price column
  mutate(total_price = quantity * price) %>%
  mutate(year = year(order.date)) %>%

  # Group by state AND year to make 12 individual plots for every state
  group_by(state, year) %>%

  # Remove some columns
  select(-quantity, -price, -order.date, -city) %>%

  # Since we grouped by state and year, we get the sales for every year and state
  summarize(sales = sum(total_price)) %>%

  mutate(sales_text = scales::dollar(sales, big.mark = ".",
                                     decimal.mark = ",",
                                     prefix = "",
                                     suffix = " €"))

# Visualization

sales_by_year_and_loc_tbl %>%

  # Setup canvas with the columns year (x-axis) and sales (y-axis)
  ggplot(aes(x = year, y = sales, fill = state)) +

  # Don't use manual colors, because facet_wrap will take care of it
  geom_col() +
  
  # Plots 12 individual figures for every state
  facet_wrap(~ state) + 

  # Note that the individual plots already have the state names as title, so legend is unneccessary
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none") +

  scale_y_continuous(labels = scales::dollar_format(big.mark = ".",
                                                    decimal.mark = ",",
                                                    prefix = "",
                                                    suffix = " €")) +
  labs(
    title    = "Revenue by year for the respective states",
    x = "Year", # Override defaults for x and y
    y = "Revenue"
  )
```


# Chapter 3: Data Acquisition

## Acquiring Spotify Data

The following code showcases how to access some data of a specific album from the Spotify Database.

```{r, fig.width=10, fig.height=7}
# load libraries
library(spotifyr)
library(tidyverse)

# get authorization token, even though most of methods call get_spotify_access_token() by themselves
access_token <- get_spotify_access_token()

# raw album data from "amo" by Bring Me The Horizon, by using the spotify id of that album
raw_data <- get_album("04mkS7FooK8fRbB626T9NR")

# select important columns
tracks_tbl <- raw_data$tracks$items %>% select(track_number, name, explicit, id, duration_ms)

tracks_tbl

# acquire the tempo (in BPM) from all tracks by accessing with their respective track id
tracks_bpm <- vector()

for(i in 1:dim(tracks_tbl)[1]) {
  tracks_bpm <- c(tracks_bpm, get_track_audio_analysis(tracks_tbl[i, 4])$track$tempo)
}

# calculate the mean bpm of all tracks
# result for that album: 132 BPM
bpm_mean = mean(tracks_bpm) 

# same for tempo confidence
tracks_bpm_conf <- vector()

for(i in 1:dim(tracks_tbl)[1]) {
  tracks_bpm_conf <- c(tracks_bpm_conf, get_track_audio_analysis(tracks_tbl[i, 4])$track$tempo_confidence)
}

tempo_tbl <- data.frame(tracks_bpm, tracks_bpm_conf)

# we could state the hypothesis that a high tempo could result in a low tempo confidence, 
# so we test the correlation of tempo and tempo confidence

# result is close to zero, so there is no strong correlation
correlation = cor(tracks_bpm, tracks_bpm_conf)


# we also try to confirm these results from a scatter plot
ggplot(tempo_tbl, aes(x =tracks_bpm, y = tracks_bpm_conf)) +
         
         geom_point(size=4, shape=4) +
  
        geom_text(label=tracks_tbl$name) + 
  
  labs(
    title = "Tempo and Tempo Confidence of albumtracks from \"amo\" by Bring Me The Horizon",
    x = "Tempo in BPM (Beats Per Minute)",
    y = "Tempo Confidence in percent"
  )

```
The scatter plot also shows no strong correlation between the track tempo and the tempo confidence.


## Acquiring Bike Data

The following code acquires bike data from rosebikes.com.

```{r}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing

# home urls
og_url_home <- "https://www.rosebikes.com"
url_home <- "https://www.rosebikes.com/bikes"

# access raw html data for home url
html_home <- read_html(url_home)

# first we need every url for all models by accessing their hrefs and glueing them into urls
model_urls <- html_home %>%

   html_nodes(css = ".catalog-navigation__link") %>%
   
   # get the specific html attribute
   html_attr('href') %>%
   
   # remove unwanted product families
   discard(.p = ~stringr::str_detect(.x,"sale|kids")) %>%

   # convert vector to tibble
   enframe(name = "Name", value = "subdirectory") %>%

   # final glueing of urls
   mutate(
    url = glue("https://www.rosebikes.com{subdirectory}")
   )


# function for acquiring bike data for every model
get_bike_data <- function(url) {
  
  bike_html <- read_html(url)
  
  # 1. obtain list of model names
  model_name_lst <- bike_html %>%
    
    html_nodes(css = ".catalog-category-bikes__title-text") %>%
    
    html_text() %>%
    
    str_remove(pattern="\\n") %>%
    str_remove(pattern="\\n") 
  
  # 2. make list of model type (which is the same for this url)
  model_type <- bike_html %>%
    html_nodes(css = ".catalog-breadcrumb__list-item-link") %>%
    html_attr("title")
  
  model_type <- model_type[length(model_type)]
  
  model_types_lst = rep(model_type, length(model_name_lst))
  
  # 3. obtain prices for every bike
  prices_lst <- bike_html %>%
    
    html_nodes(css = ".catalog-category-bikes__price-title") %>%
    
    html_text() %>% 
    
    str_remove_all(pattern="\\n") 
    
    #str_remove(pattern="from €") %>% 
    
  # return merged tibble
  data_tbl <- tibble(model_types_lst, model_name_lst, prices_lst) %>%
  
              rename(model_type = model_types_lst, model_name = model_name_lst, model_prices = prices_lst)
  
  return(data_tbl)
}

url_vector <- model_urls$url

# use map to execute function for every url in the vector
bike_data_lst <- map(url_vector, get_bike_data)

bike_data_tbl <- bind_rows(bike_data_lst)

bike_data_tbl
```

# Chapter 4: Data Wrangling

## Patent Dominance

```{r, eval=F}
library(vroom)
library(data.table)
library(tidyverse)

col_types <- list(
  id = col_character(),
  organization = col_character()
)

assignee_tbl <- vroom(
  file       = "data-science/00_data/assignee.tsv",
  delim      = "\t",
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

col_types <- list(
  patent_id = col_character(),
  assignee_id = col_character()
)

patent_assignee_tbl <- vroom(
  file       = "data-science/00_data/patent_assignee.tsv",
  delim      = "\t",
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

setDT(assignee_tbl)
setDT(patent_assignee_tbl)

setnames(assignee_tbl, "id", "assignee_id")

first_combined_data_tbl <- merge(x=assignee_tbl, y= patent_assignee_tbl, by = "assignee_id")

first_combined_data_tbl <- select(first_combined_data_tbl, assignee_id, name_first, name_last, organization, patent_id)

first_summed_patents_tbl <- first_combined_data_tbl[is.na(name_first), .N, by=organization]

first_summed_patents_tbl <- arrange(first_summed_patents_tbl, desc(first_summed_patents_tbl$N))

first_summed_patents_tbl <- first_summed_patents_tbl[1:10]
```

```{r}

first_summed_patents_tbl

```


## Recent Patent Activity

```{r, eval=F}
library(vroom)
library(data.table)
library(tidyverse)
library(tictoc)

col_types <- list(
  id = col_character(),
  date = col_date("%Y-%m-%d"),
  num_claims = col_double()
)

patent_tbl <- vroom(
  file       = "data-science/00_data/patent.tsv",
  delim      = "\t",
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

patent_tbl <- select(patent_tbl, id, date)

tic()

patent_tbl <- patent_tbl %>% separate(col = date,
                                      into = c("year", "month", "day"),
                                      sep = "-",
                                      remove = TRUE) %>%

                            mutate(year = as.numeric(year)) %>%

                            filter(year == 2019)


toc()

col_types <- list(
  id = col_character(),
  organization = col_character()
)

assignee_tbl <- vroom(
  file       = "data-science/00_data/assignee.tsv",
  delim      = "\t",
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

col_types <- list(
  patent_id = col_character(),
  assignee_id = col_character()
)

patent_assignee_tbl <- vroom(
  file       = "data-science/00_data/patent_assignee.tsv",
  delim      = "\t",
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

setDT(assignee_tbl)
setDT(patent_assignee_tbl)

setnames(assignee_tbl, "id", "assignee_id")

combined_data_tbl <- merge(x=assignee_tbl, y= patent_assignee_tbl, by = "assignee_id")


combined_data_tbl <- combined_data_tbl %>% filter(patent_id %in% patent_tbl$id)

summed_patents_tbl <- combined_data_tbl[is.na(name_first), .N, by=organization]

summed_patents_tbl <- arrange(summed_patents_tbl, desc(summed_patents_tbl$N))

```

```{r}
summed_patents_tbl[1:10]
```

```{r}
col_types<- list(
  patent_id = col_character(),
  mainclass_id = col_number(),
  sequence = col_number()
)

uspc_tbl <- vroom(
  file       = "data-science/00_data/uspc.tsv",
  delim      = "\t",
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

uspc_combined_data <- merge(x = uspc_tbl, y = combined_data_tbl, by = "patent_id")

uspc_combined_data %>%
    group_by(mainclass_id) %>%
    summarise(count = n()) %>%
    ungroup() %>%
    arrange(desc(count)) %>%
    slice(1:5)
```



# Chapter 5: Data Visualization

## COVID-19 Cummulative Cases

```{r}
library(tidyverse)
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")

grouped_covid_data_tbl <- covid_data_tbl %>% 
  
              filter(year == 2020) %>%
  
              filter(countriesAndTerritories == "Germany" | countriesAndTerritories == "Spain" |
                       countriesAndTerritories == "United_Kingdom" | countriesAndTerritories == "France" |
                       countriesAndTerritories == "United_States_of_America") %>%
  
              select(dateRep, day, month, cases, countriesAndTerritories) %>%
               
              mutate(absDay = yday(dmy(dateRep))) %>%

              arrange(absDay) %>%

              group_by(countriesAndTerritories) %>%
               
              mutate(cum_cases = cumsum(cases))

ylabel = c(0, 5, 10, 15)
xlabel = c("February, April, June, August, October, December")

grouped_covid_data_tbl %>% 
  
              ggplot(aes(x = lubridate::dmy(dateRep), y = cum_cases, color = countriesAndTerritories)) +
  
              geom_step(size=0.75) + 
  
              scale_y_continuous(limits = c(0,16e6), labels=paste0(ylabel, "M")) +
              scale_x_date(date_breaks = "2 months", date_labels = "%b") +
  
  scale_color_viridis_d(option = "A") +
  
              geom_label(data = filter(grouped_covid_data_tbl, cum_cases ==last(cum_cases), 
                                       countriesAndTerritories %in% "United_States_of_America"), 
                         aes(label = cum_cases, fill = countriesAndTerritories), hjust = "inward",
             size  = 3,
             color = RColorBrewer::brewer.pal(n = 11, name = "RdBu")[11], show.legend = FALSE) +
  
              labs(
                title = "COVID-19 confirmed cases worldwide",
                subtitle = "As of 06.12.2020, the increase in cases is very high for the USA",
                x = "Year 2020",
                y = "Cummulative Cases",
                color = "Countries"
              )
```

## COVID-19 Worldwide Death Rates
```{r}
world_map <- map_data("world")

covid_death_map <- covid_data_tbl%>%
  
    mutate(across(countriesAndTerritories, str_replace_all, "_", " ")) %>%
    
    mutate(countriesAndTerritories = case_when(
      countriesAndTerritories == "United Kingdom" ~ "UK",
      countriesAndTerritories == "United States of America" ~ "USA",
      countriesAndTerritories == "Czechia" ~ "Czech Republic",
      TRUE ~ countriesAndTerritories
    )) %>%
  
    mutate(region = countriesAndTerritories) %>%
    
    select(region, deaths, popData2019) %>%
    
    group_by(region) %>%
    
    summarise(mortalityRate = sum(deaths)/popData2019) %>%
    
    distinct(region, .keep_all = TRUE) %>%
    
    mutate(mortalityRate_perc = scales::percent(mortalityRate, accuracy = 0.001))


covid_death_map <- covid_death_map %>%
  
  left_join(world_map, by = "region") %>%
  mutate(`Mortality Rate` = mortalityRate)


covid_death_map %>% ggplot() + 
  
  geom_map(aes(x = long, y = lat, map_id = region, fill = `Mortality Rate`, 
               label = mortalityRate_perc), map = world_map) +

  scale_fill_continuous(labels = scales::percent_format(accuracy = 0.01), low = "#fc0303", high = "black") +
  
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(),
        axis.ticks.x = element_blank(), axis.ticks.y = element_blank(),
        axis.text.x = element_blank(), axis.text.y = element_blank()) +
  
  labs(
    title = "Confirmed COVID-19 deaths relative to the size of the population",
    subtitle = "More than 1.2 Million confirmed Covid-19 deaths worldwide",
    x = "",
    y = "",
    caption = "Date: 06/12/2020"
  ) 
```